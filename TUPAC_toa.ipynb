{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/TOA_causal\n"
     ]
    }
   ],
   "source": [
    "%cd ~/TOA_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from TOA.mbfdunetln import MBPFDUNet\n",
    "from ANDMask.adam_flexible_weight_decay import AdamFlexibleWeightDecay\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from TOA.train import createForwMat\n",
    "from utils.causal_utils import train,validation,testing,computing_metrics,load_traindataset,load_testdataset,load_ckp\n",
    "from utils.noncausal_utils import load_traindataset_nc,train_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batchsize = 2 #per environment\n",
    "val_percent = 440.0/2216.0 # 440 para validacion, 1776 para train\n",
    "le = 5\n",
    "epochs = 50\n",
    "#lr = 1e-3\n",
    "#wd= 1e-6\n",
    "#agreement_threshold = 0.5 \n",
    "cache_dir = '../data/'\n",
    "fecha = '280722_16'\n",
    "continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1250/16384 [00:00<00:01, 12498.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to be used: cuda\n",
      "Creating Forward Model-based Matrix without position uncertainty\n",
      "Creating SIR Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16384/16384 [00:01<00:00, 12688.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PA Matrix...\n",
      "Applying Time Derivative Operator...\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "print(f\"Device to be used: {device}\")\n",
    "\n",
    "\n",
    "##Loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "##TOA matrix\n",
    "Ao = createForwMat()\n",
    "Ao = torch.as_tensor(Ao).type(torch.float32)\n",
    "Ao = Ao.to(device=device)\n",
    "\n",
    "##Files\n",
    "ckp_last = cache_dir + 'mbfdunetln' + fecha + '.pth' # name of the file of the saved weights of the trained net\n",
    "ckp_best = cache_dir + 'mbfdunetln_best' + fecha + '.pth'\n",
    "\n",
    "#if continue_training:\n",
    "#\tmodel, optimizer, epoch0, valid_loss_min = load_ckp(ckp_last, model, optimizer)\n",
    "#    lr_scheduler = MultiStepLR(optimizer,milestones=[le * epochs * 3 // 4],gamma=0.1,last_epoch = epoch0 - 1)\n",
    "#else:\n",
    "\t#epoch0 = 0\n",
    "\t#valid_loss_min = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [11:06<2:08:20, 167.40s/it]"
     ]
    }
   ],
   "source": [
    "alphas = [1e-4,1e-3,1e-2]\n",
    "wcosts = [0,1e-6]\n",
    "bs = [2,3]\n",
    "taus = [0.4,0.8]\n",
    "\n",
    "checkpoint = {'valid_loss_min': np.inf}\n",
    "\n",
    "epoch0 = 0\n",
    "for batchsize in bs:\n",
    "    train_loaders, val_loader = load_traindataset(cache_dir,val_percent,batchsize,val_batchsize=40,le = le)\n",
    "    for wd in wcosts:\n",
    "        for lr in alphas:\n",
    "            for agreement_threshold in taus:\n",
    "                model = MBPFDUNet().to(device=device)\n",
    "                checkpoint['state_dict'] = model.state_dict()\n",
    "                optimizer = AdamFlexibleWeightDecay(model.parameters(),lr=lr,weight_decay_order='before',weight_decay=wd)\n",
    "                checkpoint['learning_rate'] = lr\n",
    "                checkpoint['weight_cost'] = wd\n",
    "                checkpoint['batchsize'] = batchsize\n",
    "                checkpoint['agreement_threshold'] = agreement_threshold\n",
    "                checkpoint['optimizer'] = optimizer.state_dict()\n",
    "                checkpoint['epoch'] = epoch0\n",
    "                lr_scheduler = MultiStepLR(optimizer,milestones=[le * epochs * 3 // 4],gamma=0.1)\n",
    "                for epoch in tqdm(range(epoch0 + 1, epochs + 1)):\n",
    "                    train(model,device,train_loaders,optimizer,n_agreement_envs=le,Ao=Ao,loss_fn=loss_fn,agreement_threshold=agreement_threshold,scheduler=lr_scheduler)\n",
    "                    checkpoint['epoch'] = epoch\n",
    "                    checkpoint['valid_loss_min'] = validation(model, device, val_loader, optimizer, loss_fn, Ao, checkpoint, ckp_last, ckp_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, best_epoch, valid_loss_min, best_lr, best_wd, best_bs, best_threshold = load_ckp(ckp_best, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from importlib import reload \n",
    "#import utils.noncausal_utils \n",
    "#reload(utils.noncausal_utils)\n",
    "#from utils.noncausal_utils import train_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_benchmark_last = cache_dir + 'benchmark' + fecha + '.pth'\n",
    "ckp_benchmark_best = cache_dir + 'benchmark_best' + fecha + '.pth'\n",
    "checkpoint_nc = {'valid_loss_min': np.inf, 'agreement_threshold' : 0.0}\n",
    "epoch0 = 0\n",
    "for batchsize in bs:\n",
    "    train_loader_nc, val_loader_nc = load_traindataset_nc(cache_dir,val_percent,batchsize*le,val_batchsize=40,le = le)\n",
    "    for lr in alphas:\n",
    "        for wd in wcosts:\n",
    "            model_nc = MBPFDUNet().to(device=device)\n",
    "            checkpoint_nc['state_dict'] = model_nc.state_dict()\n",
    "            optimizer_nc = torch.optim.Adam(model_nc.parameters(),lr=lr,weight_decay=wd)\n",
    "            checkpoint_nc['learning_rate'] = lr\n",
    "            checkpoint_nc['weight_cost'] = wd\n",
    "            checkpoint_nc['batchsize'] = batchsize\n",
    "            checkpoint_nc['optimizer'] = optimizer_nc.state_dict()\n",
    "            checkpoint_nc['epoch'] = epoch0\n",
    "            lr_scheduler_nc = MultiStepLR(optimizer_nc,milestones=[le * epochs * 3 // 4],gamma=0.1)\n",
    "            for epoch in tqdm(range(epoch0 + 1, epochs + 1)):\n",
    "                train_nc(model_nc,device,train_loader_nc,optimizer_nc,Ao=Ao,loss_fn=loss_fn,scheduler=lr_scheduler_nc)\n",
    "                checkpoin_nc['epoch'] = epoch\n",
    "                checkpoin_nc['valid_loss_min'] = validation(model_nc, device, val_loader_nc, optimizer_nc, loss_fn, Ao, checkpoint_nc, ckp_benchmark_last, ckp_benchmark_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckp_benchmark_last = cache_dir + 'benchmark' + fecha + '.pth'\n",
    "#ckp_benchmark_best = cache_dir + 'benchmark_best' + fecha + '.pth'\n",
    "#model_nc = MBPFDUNet().to(device=device)\n",
    "#optimizer_nc = torch.optim.Adam(model_nc.parameters(),lr=lr,weight_decay=wd)\n",
    "#lr_scheduler_nc = MultiStepLR(optimizer,milestones=[le * epochs * 3 // 4],gamma=0.1)\n",
    "#if continue_training:\n",
    "#    model_nc, optimizer_nc, epoch0, vlm_nc = load_ckp(ckp_benchmark_last, model_nc, optimizer_nc)\n",
    "#else:\n",
    "#    epoch0 = 0\n",
    "#    vlm_nc = np.inf\n",
    "#train_loader_nc, val_loader_nc = load_traindataset_nc(cache_dir,val_percent,batchsize*le,val_batchsize=40,le = le)\n",
    "#for epoch in tqdm(range(epoch0 + 1, epochs + 1)):\n",
    "#    train_nc(model_nc,device,train_loader_nc,optimizer_nc,Ao=Ao,loss_fn=loss_fn,scheduler=lr_scheduler_nc)\n",
    "#    vlm_nc = validation(model_nc, device, val_loader_nc, optimizer_nc, loss_fn, Ao, vlm_nc, epoch, ckp_benchmark_last, ckp_benchmark_best)\n",
    "\n",
    "model_nc, optimizer_nc, best_epoch_nc, vlm_nc lr_nc, wd_nc, bs_nc, at_nc= load_ckp(ckp_benchmark_best, model_nc, optimizer_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = load_testdataset(cache_dir)\n",
    "le_test = len(test_loaders)\n",
    "SSIM = [[] for _ in range(le_test)] \n",
    "PC = [[] for _ in range(le_test)] \n",
    "RMSE = [[] for _ in range(le_test)]\n",
    "PSNR = [[] for _ in range(le_test)]\n",
    "for j in range(le_test):\n",
    "    iterator = iter(test_loaders[j])\n",
    "    while 1:\n",
    "        try:\n",
    "            data_test = next(iterator)\n",
    "        except StopIteration:\n",
    "            break  \n",
    "        a,b,c,d=computing_metrics(data_test[0].to(\"cpu\"),data_test[1].to(\"cpu\"),Ao.to(device=\"cpu\"),model,model_nc)\n",
    "        SSIM[j].append(a)\n",
    "        PC[j].append(b)\n",
    "        RMSE[j].append(c)\n",
    "        PSNR[j].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from importlib import reload \n",
    "#import utils.causal_utils \n",
    "#reload(utils.causal_utils)\n",
    "#from utils.causal_utils import testing\n",
    "testing(np.array(SSIM),np.array(PC),np.array(RMSE),np.array(PSNR),test_loaders, Ao.to(device=\"cpu\"),model, model_nc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
